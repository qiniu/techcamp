**Task2**

**二、了解大模型**

**1、llm 范式**

LLM范式指的是大型语言模型（Large Language
Models）的应用和研究框架。这些模型通过大量数据训练，能够理解和生成自然语言，执行各种任务

**1. Prompt-based Paradigm（提示式范式）**

该范式通过构造精心设计的提示（Prompt）将任务目标融入自然语言输入中，使语言模型能够通过"续写"方式生成答案。提示可包含任务说明、示例或上下文信息。该范式适用于文本生成、分类、翻译等任务。

示例结构：

Prompt = Instruction + Input

模型输出：Answer

**2. Chat-based Paradigm / ChatML 格式**

该范式采用多轮对话结构，构造系统（System）、用户（User）、助手（Assistant）三类角色信息，实现上下文管理与对话行为建模。适合多轮问答、助手交互等场景，常用于
ChatGPT、Claude 等模型中。

消息格式（ChatML）：

  -----------------------------------------------------------------------
  Plain Text\
  \<\|system\|\>: You are a helpful assistant.\
  \<\|user\|\>: What is the capital of France?\
  \<\|assistant\|\>: The capital of France is Paris.

  -----------------------------------------------------------------------

**3. Retrieval-Augmented Generation (RAG) Paradigm**

RAG就是结合信息检索和生成式模型的技术。主要流程包括两个核心环节：

检索：基于用户的输入、从外部知识库（如数据库、文档、网页）检索与问题相关的信息。通常使用向量化表示和向量数据库进行语义匹配。将知识库中的文档进行预处理、分块、清洗并转换为向量表示、存储在向量数据库中。常用的如
Faiss、Milvus等向量数据库存储所有文档向量。用户提问后、对问题进行向量化、并在数据库中执行最近邻搜索、找出语义最相近的
N 条内容

然后就是增强：也可以说是构建 Prompt

1.将检索到的信息作为上下文、输入给生成模型（如 GPT）。

2.相比纯生成模型、RAG 能引用真实数据、减少幻觉（胡编乱造）

最后就是由将增强后的上下文输入到大型语言模型、综合已有上下文生成最终生成最终的回答或内容。

一句话总结: RAG = 向量搜索引擎 + 大模型、让 AI 回答更靠谱、减少幻觉

RAG范式引入外部知识检索模块，将查询输入转化为搜索请求，检索相关文档后与原始查询共同构成模型输入，从而提高语言模型的事实性与扩展能力。

流程包括：

查询 Query；

向向量数据库或文档库检索相关上下文；

构造 Prompt（Query + Retrieved Content）；

输入至 LLM，生成响应。

**4. Chain-of-Thought (CoT) Paradigm**

该范式通过引导模型生成**逐步推理过程**（Reasoning
Trace），提高模型在复杂推理与数学题等任务中的准确率。通常配合提示示例（Few-shot）构建推理链。

示例 Prompt："Q:
如果小明有3个苹果，每个苹果3元，小明有多少钱？请一步步推理。"

**5. Tool-augmented Paradigm / ReAct / Agent Paradigm**

该范式允许语言模型在推理过程中调用外部工具（如搜索、计算器、数据库等），实现感知---思考---行动---反思的能力循环。ReAct
框架将"思维轨迹 + 行动调用"作为交替输出结构。

典型流程：

  -----------------------------------------------------------------------
  Plain Text\
  Thought → Action → Observation → (loop) → Final Answer

  -----------------------------------------------------------------------

**6. MCP**

MCP（模型上下文协议）是为大型语言模型提供的一个统一标准化接口、让AI能够无缝连接各种外部数据源和工具。

可以将它比作AI世界的USB接口---只要遵循这个协议标准、任何数据源或工具都能与语言模型实现即插即用比如说传统的AI只能依赖预训练的静态知识、无法获取实时数据。而通过MCP，模型可以动态访问最新信息、比如查询搜索引擎、读取本地文件、调用第三方API、甚至直接操作各种工具库。比如说可以访问Github、IDEA

这个协议最大的价值是标准化、它是MCP的核心价值 -
你不需要为每个AI模型和每个工具之间的连接编写专门的代码、只要双方都支持MCP协议、它们就能自动\"对话\"。这大大简化了系统集成、降低了开发成本、也提高了系统的可扩展性

总结就是 MCP 创建一个通用标准、使 AI
应用程序的开发和集成变得更加简单和统一

**2、openai 范式**

"OpenAI 范式 API"主要指的是 OpenAI
提供的一种**统一的调用方式**，支持各种任务，比如对话、代码生成、问答、总结等。它围绕一个核心范式构建：**"以对话为中心的消息格式（ChatML） +
单一接口完成多任务"**。

**（1）为什么叫"范式"？**

传统 API（如补全、问答、翻译）每个任务要用不同接口或模型，而 OpenAI
提供的是一种**统一思路**：

所有任务都通过"对话"形式表达

所有行为都通过 prompt 实现（无需额外训练）

所有调用都走一个 Chat 接口

**（2）核心设计范式：模型即服务 (Model-as-a-Service, MaaS)**

OpenAI
范式的核心思想是，将极其复杂和庞大的预训练模型封装为一种可通过网络调用的**云服务**。开发者无需关心模型的训练、部署、维护和扩展，只需通过简单的
API 请求，就能在自己的应用程序中集成顶尖的 AI 能力。

其主要特点包括：

**无状态 (Stateless)：** 每个 API
请求都是独立的。模型本身不记录前一次对话的上下文。如果您需要进行多轮对话，必须在每次请求中**自己携带完整的对话历史**。这是最关键也最核心的一个范式。

**以文本（或多模态内容）为中心：**
交互的主要媒介是文本。您通过精心设计的"提示"（Prompt）来引导模型生成所需的回应。现在也扩展到了图片、音频等多模态内容。

**统一的接口：**
无论是执行翻译、写代码、回答问题还是生成创意，都使用相似的 API
结构。通过改变输入内容（messages）和参数（parameters）来控制模型的行为

**3、chatml**

**（1）简介**

ChatML 是 OpenAI 为其对话式大语言模型（如
GPT-4、GPT-4o）设计的一种**对话格式标注语言（Chat Markup
Language）**，用于告诉模型哪些部分是用户说的、哪些部分是系统或助手说的，甚至是一些隐藏的指令（system
prompt）。它是 prompt 工程中的一种关键技术。

**（2）定义**

ChatML
是一种**轻量级的标记语言格式**，你可以理解为专门给多轮对话"打标签"的规则，便于模型理解不同角色之间的交流。

  -----------------------------------------------------------------------
  Plain Text\
  \<\|role\|\>\
  message content

  -----------------------------------------------------------------------

**（3）作用**

是 prompt 工程的基础之一

支持多轮记忆、角色设定、函数调用

是构建 Agent（智能体）、RAG（检索增强生成）、Tool-augmented LLM
的关键拼图

**4、prompt**

**（1）定义**

**Prompt**
本质上是**你给大语言模型（LLM）下的指令**，用人类语言（自然语言）写的。可以简单理解为：

  -----------------------------------------------------------------------
  "让模型干什么的那段话。"

  -----------------------------------------------------------------------

它是人和大模型之间的桥梁，决定了模型的行为、语气、输出格式和内容质量。

**（2）Prompt 工程（Prompt Engineering）**

随着大模型能力越来越强，「怎么问比问什么更重要」。因此，衍生出一个专门领域叫：**Prompt
Engineering（提示工程）**

它关注的是：

如何设计 Prompt 让模型更稳定、可控

如何构造 Prompt 让模型在不同任务上表现更好

如何组合 Prompt 与工具 / 数据库 / 多轮交互等

**（3）Prompt 的常见设计策略**

![](media/image1.png)

**点击图片可查看完整电子表格**

**（4）底层工作机制**

在 OpenAI、Anthropic 等模型中，Prompt 实际上是序列的一部分，LLM
是把它和历史记录一同作为"输入上下文"，再生成"接下来最有可能的内容"。

对于开发者，关键关注点是：

Prompt 的**token 数量**会影响成本和性能

Prompt 写法影响模型的"理解"和"预期"

Prompt 可以被缓存（Embedding + 检索增强）

**5、workflow**

**（1）定义**

**Workflow（工作流）**：指的是**一系列按特定顺序执行的任务流程**，每一步都有输入、处理、输出，多个步骤连接起来形成完整业务逻辑。

简单理解就是：

  -----------------------------------------------------------------------
  "一张流程图：谁做什么、何时做、做完之后交给谁。"

  -----------------------------------------------------------------------

**（2）通用结构**

![](media/image2.png)

**点击图片可查看完整电子表格**

**（3）和 Prompt 的关系？**

Prompt 是某一"节点"的具体执行指令。

而 Workflow 是：

  -----------------------------------------------------------------------
  管控这些 Prompt、工具、逻辑判断、数据流动的一整条"管道"。

  -----------------------------------------------------------------------

**6、mcp**

**（1）定义**

MCP（Model Context
Protocol，模型上下文协议）是一种用于标准化大模型与外部系统之间交互的协议。它的核心目标是为大语言模型（如
GPT、LLM
等）提供一种统一、结构化的方式来获取外部知识、调用工具、访问数据库或与其他服务集成。

**类比理解：**

**Prompt**：是指令

**Agent**：是执行者

**MCP**：是搭建这个"智能系统"的方法论

**（2）主要特点**

**标准化接口**：MCP 定义了一套通用的 API
和数据格式，使不同的模型和工具可以通过一致的方式进行通信，降低了集成和扩展的难度。

**上下文管理**：MCP
强调"上下文"的概念，允许模型在推理过程中动态获取、更新和管理外部信息，从而提升模型的智能和实用性。

**工具调用**：通过
MCP，模型可以安全、可控地调用外部工具（如搜索引擎、计算服务、数据库等），实现"工具增强型智能"。

**可扩展性**：MCP
协议支持多种扩展方式，便于开发者根据实际需求自定义接口、数据结构和权限控制。

**（3）开发实例**

  -----------------------------------------------------------------------
  TypeScript\
  Agent {\
  Modules: \[\
  MemoryModule,\
  ToolUseModule,\
  PlanningModule,\
  LanguageModule\
  \],\
  Runtime: Workflow + 状态控制 + 可观测性\
  }

  -----------------------------------------------------------------------

每个模块负责单一职责，比如：

**MemoryModule**：上下文记忆/历史摘要

**ToolUseModule**：结构化调用插件/API

**LanguageModule**：Prompt 生成、解析

**PlanningModule**：拆解子任务、规划执行顺序

然后用一个 "执行框架" 来编排这些模块（可用 LangGraph、AutoGen、CrewAI
实现）。

**（4）mcp 和其他概念的关系**

![](media/image3.png)

**点击图片可查看完整电子表格**

**7、rag**

**（1）定义**

RAG 全称是：

  -----------------------------------------------------------------------
  **Retrieval-Augmented Generation 检索增强生成**

  -----------------------------------------------------------------------

通俗解释就是：

  ------------------------------------------------------------------------------------------
  **把外部知识库中的内容检索出来，作为上下文交给大语言模型，让它基于这些知识来回答问题**。

  ------------------------------------------------------------------------------------------

**（2）RAG 的作用**

**引入外部知识**（你自己的数据、文档、网页）

**降低幻觉风险**（模型参考真实内容）

**即时更新知识**（不需要重训模型）

**（3）RAG 和微调的区别**

![](media/image4.png)

**点击图片可查看完整电子表格**
